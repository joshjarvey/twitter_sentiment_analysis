{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#############################################################################################################################################################\n##                                                                                                                                                         ##\n## This project uses the Tweepy wrapper package to pull Twitter data on a given subject using the Twitter API,                                             ##\n## \"cleans\" the tweets by removing any URLs using a regular expression function,                                                                           ##\n## and finally conducts sentiment analysis using the TextBlob package which is loaded into a dataframe. Neutral sentiment values of 0 are \"filtered out\",  ##\n## and then a histogram of the data is plotted to display distribution visually so the summary can be easily interpreted.                                  ##\n##                                                                                                                                                         ##\n## Note: Twitter API credentials must be loaded in via a .json file in order to connect to the Twitter API.                                                ##\n##                                                                                                                                                         ##\n## Credit goes to: Martha Morrissey, Leah Wasser, Jeremey Diaz, and Jenny Palomino.                                                                        ##\n## https://www.earthdatascience.org/courses/earth-analytics-python/using-apis-natural-language-processing-twitter/analyze-tweet-sentiments-in-python/      ##\n##                                                                                                                                                         ##\n#############################################################################################################################################################\n\nimport numpy as np \nimport pandas as pd\nimport json\nimport tweepy\nimport re\nfrom textblob import TextBlob\nimport matplotlib.pyplot as plt\nimport datetime\nimport os\n\n\n#::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n#:: Setting initinal parameters and creating the twitter API connection :::\n#::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n\n\n## Setting search parameters with the term and date. By default, using today's date. ##\nsearch_term = \"#Trump\"\nsearch_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n\n## Opening twitter API credentials file - JSON format. ##\nwith open('/kaggle/input/twitter/twitter_credentials.json') as f:\n    credentials = json.load(f)    \n\n## Connecting to the Twitter API and passing the credentials. ##\nauth = tweepy.OAuthHandler(credentials['consumer_key'],credentials['consumer_secret'])\nauth.set_access_token(credentials['access_token'],credentials['access_token_secret'])\napi = tweepy.API(auth)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n#:: Pulling, parsing, scrubbing, and prepping tweet data to be passed into TextBlob package :::\n#::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n\n\n## this function accepts a text string and parses for a URL to be removed using the regular expressions package. It returns a string back to the caller with the URL removed.\ndef remove_url(text):\n    return \" \".join(re.sub(\"([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \"\", text).split())\n\n## setting the string to be passed into the cursor call, includes the filtering of retweets. ##\nsearch_string = search_term +\" -filter:retweets\"\n\n## creating the tweepy cursor iterator object, and executing the search. ##\ntweet_block = tweepy.Cursor(api.search,q=search_term,lang=\"en\",since=search_date).items(1000)\n\n## iterates through each tweet object and passes the text to the remove_url function, to remove URLS from the text. ## \ntweets_no_urls = [remove_url(tweet.text) for tweet in tweet_block]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n#:: Conducting the sentiment analysis via the TextBlob package and displaying it into a visual :::\n#:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n\n\n## passing each tweet into the TextBlob package for sentiment analysis. ##\ntextblob_tweet = [TextBlob(tweet) for tweet in tweets_no_urls]\n\n## parsing the text of each tweet into a string, and then determining the polarity. ##\nsentiment_values = [[tweet.sentiment.polarity, str(tweet)] for tweet in textblob_tweet]\n\n## creating a dataframe of the polarity and tweet. ##\ntweet_df = pd.DataFrame(sentiment_values, columns=[\"Polarity\",\"Tweet\"])\n\n## removing \"neutral\" tweets from the dataframe. ##\ntweet_df = tweet_df[tweet_df.Polarity != 0]\n\n## displaying the distribution of negative vs. positive tweets via a histogram. ##\ntweet_df.hist(bins=[-1.00,-0.75,-0.50,-0.25,0,0.25,0.50,0.75,1.00])\nplt.title(\"Sentiments from Tweets on \"+ search_term + \" \" + today)\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}